---
title: "Heldout CRISPR data benchmarking"
author: "Andreas R. Gschwind"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setupDocument, include=FALSE}
# set output html chunk options
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# set random seed
set.seed(snakemake@params$seed)

# number for iterations (resamplings) for bootstrapping analyses
boot_iter <- snakemake@params$bootstrap_iterations
```

## Goal
Perform CRISPR benchmarking analyses on held-out data.
```{r attachPackages, message=FALSE, warning=FALSE}
# attach required packages
library(ROCR)
library(caTools)
library(tidyverse)
library(cowplot)
library(DT)

# required functions
crispr_benchmark_dir <- snakemake@config$crispr_benchmark_dir
source(file.path(crispr_benchmark_dir, "workflow/scripts/crisprComparisonLoadInputData.R"))
source(file.path(crispr_benchmark_dir, "workflow/scripts/crisprComparisonPlotFunctions.R"))
source(file.path(crispr_benchmark_dir, "workflow/scripts/crisprComparisonBootstrapFunctions.R"))
```

***

## Load and process input data
Load and process output from CRISPR benchmarking pipeline.
```{r prepareData}
# load pred_config file
pred_config <- importPredConfig(snakemake@input$pred_config, expr = FALSE,
                                include_col = "crispr_main_benchmarks")

# load CRISPR data merged with predictors and E-G pair annotations
merged_training <- read_tsv(snakemake@input$merged_training, show_col_types = FALSE)
merged_heldout <- read_tsv(snakemake@input$merged_heldout, show_col_types = FALSE)

# process merged data for benchmarking analyses, including filtering for ValidConnection == TRUE
merged_training <- processMergedData(merged_training, pred_config = pred_config,
                                     filter_valid_connections = TRUE,
                                     include_missing_predictions = TRUE)

merged_heldout <- processMergedData(merged_heldout, pred_config = pred_config,
                                    filter_valid_connections = TRUE,
                                    include_missing_predictions = TRUE)

# get models available for both the training and held out CRISPR data
models <- intersect(merged_training$pred_uid, merged_heldout$pred_uid)

# only retain data on models to include in analyses
merged_training <- filter(merged_training, pred_uid %in% models)
merged_heldout <- filter(merged_heldout, pred_uid %in% models)

# get thresholds for all predictors
thresholds <- pred_config %>% 
  filter(pred_uid %in% models) %>% 
  getThresholdValues(threshold_col = "alpha")

# get colors for all models to plot
pred_colors <- pred_config %>% 
  filter(pred_uid %in% models) %>% 
  select(pred_name_long, color) %>% 
  deframe()

# get predictor names to use in plots
pred_names <- pred_config %>% 
  filter(pred_uid %in% models) %>% 
  select(id = pred_uid, pred_name_long)
```

***

## Weighted performance
Calculate performance weighted by probability of E-G pairs being direct effects for canonical
enhancers.

#### Select enhancers to include
Any positives that have non-canonical enhancer chromatin signatures (No H3K27ac, CTCF element,
H3K27me3 element), or have a very low CRISPR effect size were removed from both the training and
held-out CRISPR data for this analysis.

#### Calculate weighted performance
Calculate weighted performance metrics for both training and held-out CRISPR datasets.
```{r computePerformance}
# convert merged data bootstrapping format
merged_training_bs <- convertMergedForBootstrap(merged_training, pred_config = pred_config,
                                                weight_col = "direct_vs_indirect_negative")
merged_heldout_bs <- convertMergedForBootstrap(merged_heldout, pred_config = pred_config,
                                                weight_col = "direct_vs_indirect_negative")

# compute weighted AUPRC including confidence intervals
weighted_auprc_training <- bootstrapPerformanceIntervals(merged_training_bs, metric = "auprc",
                                                         weighted = TRUE, R = boot_iter)
weighted_auprc_heldout <- bootstrapPerformanceIntervals(merged_heldout_bs, metric = "auprc",
                                                        weighted = TRUE, R = boot_iter)

# compute weighted Precision at threshold including confidence intervals
weighted_precision_training <- bootstrapPerformanceIntervals(merged_training_bs,
                                                             metric = "precision",
                                                             thresholds = thresholds,
                                                             weighted = TRUE, R = boot_iter)
weighted_precision_heldout <- bootstrapPerformanceIntervals(merged_heldout_bs,
                                                            metric = "precision",
                                                            thresholds = thresholds,
                                                            weighted = TRUE, R = boot_iter)

# compute weighted Reacll at threshold including confidence intervals
weighted_recall_training <- bootstrapPerformanceIntervals(merged_training_bs,
                                                          metric = "recall",
                                                          thresholds = thresholds,
                                                          weighted = TRUE, R = boot_iter)
weighted_recall_heldout <- bootstrapPerformanceIntervals(merged_heldout_bs,
                                                         metric = "recall",
                                                         thresholds = thresholds,
                                                         weighted = TRUE, R = boot_iter)
```

```{r combinePerformance}
# make predictor names a factor ordered by AUPRC on training dataset
pred_names_ordered <- weighted_auprc_heldout %>% 
  left_join(pred_names, by = "id") %>% 
  mutate(pred_name_long = fct_reorder(pred_name_long, .x = full, .desc = TRUE)) %>% 
  select(id, pred_name_long)

# combine AUPRC on training and held-out CRISPR data
weighted_auprc <- bind_rows(Training = weighted_auprc_training, `Held-out` = weighted_auprc_heldout,
                            .id = "dataset")

# combine Precision on training and held-out CRISPR data
weighted_precision <- bind_rows(Training = weighted_precision_training,
                                `Held-out` = weighted_precision_heldout, .id = "dataset")

# combine Recall on training and held-out CRISPR data
weighted_recall <- bind_rows(Training = weighted_recall_training, `Held-out` = weighted_recall_heldout,
                             .id = "dataset")

# add predictor names for plots
weighted_auprc <- left_join(weighted_auprc, pred_names_ordered, by = "id")
weighted_precision <- left_join(weighted_precision, pred_names_ordered, by = "id")
weighted_recall <- left_join(weighted_recall, pred_names_ordered, by = "id")
```

***

## Main figure panel
Plot weighted AUPRC on held-out data to show overall performance of ENCODE-rE2G on unseen CRISPR
data.
```{r, fig.height=5.75, fig.width=4}
# plot weighted AUPRC on held-out CRISPR data
ggplot(filter(weighted_auprc, dataset == "Held-out"),
       aes(x = pred_name_long, y = full, fill = pred_name_long)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.4, color = "black") +
  labs(y = "Weighted AUPRC", title = "Predicting CRISPR links\nHeld-out test datasets") +
  scale_fill_manual(values = pred_colors) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(),
        legend.position = "none")

# save plot to pdf
ggsave("results/manuscript/plots/main_fig2d_crispr_performance_heldout.pdf", height = 5.75,
       width = 4)
```

Significance of differences in weighted AUPRC between predictors on held-out CRISPR data: 
```{r}
# compute significance between predictors
delta_auprc_heldout <- bootstrapDeltaPerformance(merged_heldout_bs, metric = "auprc",
                                                 weighted = TRUE, R = boot_iter)

datatable(delta_auprc_heldout)
```

***

## Plot other performance metrics
Plot weighted Precision and Recall at threshold for held-out CRISPR dataset.
```{r}
# extract data on weighted Precision on held-out data and order predictors according to Precision
weighted_precision_heldout <- weighted_precision %>% 
  filter(dataset == "Held-out") %>% 
  mutate(pred_name_long = fct_reorder(pred_name_long, .x = full, .desc = TRUE))

# extract data on weighted Recall on held-out data and order predictors according to Recall
weighted_recall_heldout <- weighted_recall %>% 
  filter(dataset == "Held-out") %>% 
  mutate(pred_name_long = fct_reorder(pred_name_long, .x = full, .desc = TRUE))
```

```{r, fig.height=5, fig.width=8}
# plot weighted Precision on held-out CRISPR data
plot_precision_heldout <- ggplot(weighted_precision_heldout,
       aes(x = pred_name_long, y = full, fill = pred_name_long)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.4, color = "black") +
  labs(y = "Weighted Precision", title = "Precision\nHeld-out CRISPR data") +
  scale_fill_manual(values = pred_colors) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(),
        legend.position = "none")

# plot weighted Recall on held-out CRISPR data
plot_recall_heldout <- ggplot(weighted_recall_heldout,
       aes(x = pred_name_long, y = full, fill = pred_name_long)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.4, color = "black") +
  labs(y = "Weighted Recall", title = "Recall\nHeld-out CRISPR data") +
  scale_fill_manual(values = pred_colors) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), axis.title.x = element_blank(),
        legend.position = "none")

# arrange plots into one figure
plot_grid(plot_precision_heldout, plot_recall_heldout, nrow = 1)

# save plot to pdf
ggsave("results/manuscript/plots/extended_data_fig3f_crispr_performance_heldout.pdf", height = 5,
       width = 8)
```

Significance of differences in weighted Precision between predictors on held-out CRISPR data: 
```{r}
# compute significance between predictors
delta_precision_heldout <- bootstrapDeltaPerformance(merged_heldout_bs, metric = "precision",
                                                     thresholds = thresholds, weighted = TRUE,
                                                     R = boot_iter)

datatable(delta_precision_heldout)
```

Significance of differences in weighted Recall between predictors on held-out CRISPR data: 
```{r}
# compute significance between predictors
delta_recall_heldout <- bootstrapDeltaPerformance(merged_heldout_bs, metric = "recall",
                                                  thresholds = thresholds, weighted = TRUE,
                                                  R = boot_iter)

datatable(delta_recall_heldout)
```

```{r}
# combine all significance test on held-out data into one table and save to output .tsv file
delta_perf_heldout <- bind_rows(delta_auprc_heldout, delta_precision_heldout, delta_recall_heldout)
write_tsv(delta_perf_heldout,
          file = "results/manuscript/tables/crispr_heldout_pairwise_comparisons.tsv")
```

***

## Weighted Precision-Recall Curve
Compute and plot weighted Precision-Recall Curve.
```{r weightedPrcFunctions}
# function to compute precision-recall curve for one predictor
compute_pr_curve_one_pred <- function(p, pos_col, weight_col = NULL) {
  
  # compute PR table
  if (is.null(weight_col)) {
    pr <- pr_curve(p, truth = !!sym(pos_col), pred_value, event_level = "second")
  } else {
    pr <- pr_curve(p, truth = !!sym(pos_col), pred_value, event_level = "second",
                   case_weights = !!sym(weight_col))
  }
  
  # reformat to make output consitent with CRISPR benchmarking pipeline
  pr <- select(pr, alpha = .threshold, precision, recall)
  pr[1, "precision"] <- NaN
  pr <- as.data.table(pr)
  
  return(pr)
  
}

# redefine function to compute precision-recall curve that allows calculating weighted performance
calcPRCurves <- function(df, pred_config, pos_col = "Regulated", weight_col = NULL) {
  
  # conver positive column to factor
  df[[pos_col]] <- factor(df[[pos_col]], levels = c(FALSE, TRUE))
  
  # split into list for lapply
  df_split <- split(df, f = df$pred_uid)
  
  # get inverse predictors
  inverse_predictors <- pred_config %>% 
    select(pred_uid, inverse_predictor) %>% 
    deframe()
  
  # multiply inverse predictors by -1 so that higher value corresponds to higher score
  inverse_predictors <- inverse_predictors[names(df_split)]  # same as predictors for cell type
  df_split <- mapply(FUN = function(pred, inv_pred) {
    inv_multiplier <- ifelse(inv_pred, -1, 1)
    pred$pred_value <- pred$pred_value * inv_multiplier
    return(pred)
  }, df_split, inverse_predictors, SIMPLIFY = FALSE)
  
  # compute precision-recall performance for each predictor
  pr <- lapply(df_split, FUN = function(p){
    compute_pr_curve_one_pred(p, pos_col = pos_col, weight_col = weight_col)
  })
  
  # convert to table and calculate F1
  pr_df <- rbindlist(pr, idcol = "pred_uid")
  pr_df$F1 <- with(pr_df, 2 / ((1 / precision) + (1 / recall)))
  
  return(pr_df)
  
}
```

```{r plotWeightedPRC, fig.height=4, fig.width=6.25}
# compute weighted precision-recall table
pr <- calcPRCurves(merged_heldout, pred_config = pred_config, pos_col = "Regulated",
                   weight_col = "direct_vs_indirect_negative")

# calculate number and percentage of experimental true positives in the CRISPR dataset
n_pos <- calcNPos(merged_heldout, pos_col = "Regulated")
pct_pos <- calcPctPos(merged_heldout, pos_col = "Regulated")

# calculate the nunbers of weighted positive and negative pairs
crispr_reg_pairs <- merged_heldout %>% 
  select(name, Regulated, direct_vs_indirect_negative) %>% 
  distinct() %>% 
  group_by(Regulated) %>% 
  summarize(pairs = n(), weighted_pairs = sum(direct_vs_indirect_negative))
  
# define x-axis label with weighted number of positives 
x_axis_label <- crispr_reg_pairs %>% 
  filter(Regulated == TRUE) %>% 
  pull(weighted_pairs) %>% 
  round(digits = 2) %>% 
  paste0("Weighted Recall (n=", ., ")")

# define title for PRC plot
title <- "Predicting CRISPR links\nHeld-out test datasets"

# make precision recall plot
makePRCurvePlot(pr, n_pos = n_pos, pct_pos = pct_pos, plot_name = title, 
                pred_config = pred_config, min_sensitivity = 0.7, line_width = 0.8,
                point_size = 3, text_size = 15, colors = pred_colors) +
  labs(x = x_axis_label, y = "Weighted Precision") + 
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "black") +
  theme_classic()

# save plot to pdf
ggsave("results/manuscript/plots/extended_data_fig3e_crispr_prc_heldout.pdf", height = 4,
       width = 6.25)
```

***

## Compare performance on training vs held-out data
Plot AUPRC, Precision and Recall on both training and held-out CRISPR datasets.
```{r plotAllPerformance}
# combine AUPRC, Precision and Recall into one table
perf_plot <- bind_rows(auprc = weighted_auprc, precision = weighted_precision,
                       recall = weighted_recall) %>% 
  mutate(pred_name_long = fct_rev(pred_name_long))

# add an additional fill group to the performance table
perf_plot <- perf_plot %>% 
  mutate(fill_group = if_else(dataset == "Held-out", true = pred_name_long, false = "Training"))

# get fill colors, which includes information on CRISPR dataset type
fill_colors <- c(Training = NA_character_, pred_colors)

# plot AUPRC for each predictor both on the training and the held-out CRISPR data 
plot_auprc <- ggplot(filter(perf_plot, metric == "auprc"),
                     aes(y = pred_name_long, x = full, color = pred_name_long, fill = fill_group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(xmin = lower, xmax = upper), position = position_dodge(0.9), width = 0.4,
                color = "black") +
  labs(title = "AUPRC", x = "Weighted AUPRC", fill = "Predictor") +
  scale_color_manual(values = pred_colors) +
  scale_fill_manual(values = fill_colors, na.value = NA) +
  scale_x_continuous(limits = c(0, 1)) +
  guides(color = "none") +
  theme_classic() +
  theme(axis.title.y = element_blank(), text = element_text(size = 13))

# plot Precision for each predictor both on the training and the held-out CRISPR data 
plot_precision <- ggplot(filter(perf_plot, metric == "precision"),
                         aes(y = pred_name_long, x = full, color = pred_name_long,
                             fill = fill_group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(xmin = lower, xmax = upper), position = position_dodge(0.9), width = 0.4,
                color = "black") +
  labs(title = "Precision at threshold", x = "Weighted Precision", fill = "Predictor") +
  scale_color_manual(values = pred_colors) +
  scale_fill_manual(values = fill_colors, na.value = NA) +
  scale_x_continuous(limits = c(0, 1)) +
  guides(color = "none") +
  theme_classic() +
  theme(axis.title.y = element_blank(), text = element_text(size = 13))

# plot Recall for each predictor both on the training and the held-out CRISPR data 
plot_recall <- ggplot(filter(perf_plot, metric == "recall"),
                      aes(y = pred_name_long, x = full, color = pred_name_long,
                          fill = fill_group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_errorbar(aes(xmin = lower, xmax = upper), position = position_dodge(0.9), width = 0.4,
                color = "black") +
  labs(title = "Recall at threshold", x = "Weighted Recall", fill = "Predictor") +
  scale_color_manual(values = pred_colors) +
  scale_fill_manual(values = fill_colors, na.value = NA) +
  scale_x_continuous(limits = c(0, 1)) +
  guides(color = "none") +
  theme_classic() +
  theme(axis.title.y = element_blank(), text = element_text(size = 13))
```

```{r, fig.height=4, fig.width=15}
# arrange plots into one figure
plot_grid(
  plot_auprc + theme(legend.position = "none"),
  plot_precision + theme(legend.position = "none", axis.text.y = element_blank()),
  plot_recall + theme(axis.text.y = element_blank()),
  nrow = 1, rel_widths = c(1, 0.59, 1.13)
)

# save plot to pdf
ggsave("results/manuscript/plots/extended_data_fig3g_all_crispr_performance_heldout.pdf",
       height = 4, width = 15)
```

Significant differences between datasets in AUPRC:
```{r significanceAUPRC}
# compute bootstrapped AUPRC differences between training and held-out data
delta_auprc <- bootstrapDeltaPerformanceDatasets(merged_training_bs, merged_heldout_bs,
                                                 metric = "auprc", weighted = TRUE, R = boot_iter)

datatable(delta_auprc)
```

Significant differences between datasets in Precision:
```{r significancePrecision}
# compute bootstrapped Precision differences between training and held-out data
delta_precision <- bootstrapDeltaPerformanceDatasets(merged_training_bs, merged_heldout_bs,
                                                     metric = "precision", thresholds = thresholds,
                                                     weighted = TRUE, R = boot_iter)

datatable(delta_precision)
```

Significant differences between datasets in Recall:
```{r significanceRecall}
# compute bootstrapped Precision differences between training and held-out data
delta_recall <- bootstrapDeltaPerformanceDatasets(merged_training_bs, merged_heldout_bs,
                                                  metric = "recall", thresholds = thresholds,
                                                  weighted = TRUE, R = boot_iter)

datatable(delta_recall)
```

```{r}
# combine all significance test on held-out data into one table and save to output .tsv file
delta_perf_training_vs_heldout <- bind_rows(delta_auprc, delta_precision, delta_recall)
write_tsv(delta_perf_training_vs_heldout,
          file = "results/manuscript/tables/crispr_training_vs_heldout_comparisons.tsv")
```

***

## Session information
This document was produced using following packages:
```{r sessionInfo}
sessionInfo()
```
